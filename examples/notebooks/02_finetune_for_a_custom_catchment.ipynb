{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from hydroml.utils import helpers as h\n",
    "from hydroml.training.finetune import run_finetune_from_timeseries\n",
    "from hydroml.workflow.prediction import run_hydrological_simulation\n",
    "from hydroml.evaluation.metrics import Metrics   \n",
    "from hydroml.config.config import load_config\n",
    "\n",
    "def get_metrics(ds):\n",
    "    metrics = Metrics(ds['y'], ds['prediction']).all_metrics().to_dataframe().reset_index().drop(columns=['catchment_id', 'lead_time', 'feature'])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example we need a trained model.\n",
    "model_path = Path('../sample_data/model/version_0')\n",
    "\n",
    "# we need to convert the transform_parameter_path to an absolute path so all the finetuned models\n",
    "# to refer to the same parameters and do not calculate a new one for each catchment.\n",
    "transform_parameter_path = (model_path / 'params.yaml').absolute()\n",
    "\n",
    "\n",
    "catchment_id = '401208'\n",
    "dynamic_data=pd.read_csv(f'../sample_data/{catchment_id}.csv', index_col=0, parse_dates=True)\n",
    "static_data=h.read_json(f'../sample_data/{catchment_id}_attributes.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into calibration and validation periods\n",
    "We extract the calibration and validation periods from the config file and use them to split our data.\n",
    "This ensures we use the same periods that were used during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydroml.config.config import load_config\n",
    "\n",
    "\n",
    "config = load_config(model_path / 'config.yaml')\n",
    "cal_periods = config.cal['periods']\n",
    "val_periods = config.val['periods']\n",
    "\n",
    "cal_dynamic_data = pd.concat([dynamic_data.loc[s:e] for s, e in cal_periods])\n",
    "val_dynamic_data = pd.concat([dynamic_data.loc[s:e] for s, e in val_periods])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the simulation\n",
    "For benchmarking we run the original/pretrained model first.\n",
    "\n",
    "We can easily adjust the config parameters for the simulation by passing them as kwargs to the run_hydrological_simulation function here we need to change the device to cpu and pass the transform_parameter_path to the simulation so it uses the same parameters as the finetuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'transform_parameter_path': transform_parameter_path,\n",
    "          'device': 'cpu', \n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data: loading transform parameters from p:\\work\\sho108\\hydroml\\examples\\notebooks\\..\\sample_data\\model\\version_0\\params.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sho108\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hydroml-dFLAodHf-py3.11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:617: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "simulation_using_original_model = run_hydrological_simulation(model_path, val_dynamic_data, static_data, catchment_id, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finetune the model for the catchment we are interested in using the calibration data. Then we can run the simulation using the finetuned model for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params.yaml\n",
      "Transforming data: loading transform parameters from ..\\sample_data\\model\\version_0\\params.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sho108\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hydroml-dFLAodHf-py3.11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:617: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\sho108\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hydroml-dFLAodHf-py3.11\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "c:\\Users\\sho108\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hydroml-dFLAodHf-py3.11\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory \\\\fs1-cbr.nexus.csiro.au\\{ev-ca-macq}\\work\\sho108\\hydroml\\examples\\sample_data\\model\\version_0\\finetune_all\\401208\\241217111535_67d6 exists and is not empty.\n",
      "\n",
      "  | Name              | Type       | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | static_embedding  | Linear     | 15     | train\n",
      "1 | dynamic_embedding | Linear     | 6      | train\n",
      "2 | lstm              | LSTM       | 266 K  | train\n",
      "3 | dropout           | Identity   | 0      | train\n",
      "4 | head              | Sequential | 2.6 K  | train\n",
      "---------------------------------------------------------\n",
      "268 K     Trainable params\n",
      "0         Non-trainable params\n",
      "268 K     Total params\n",
      "1.075     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\sample_data\\model\\version_0\\finetune_all\\401208\\241217111535_67d6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269a885d8cee4c9a9532db8846b49e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data: loading transform parameters from p:\\work\\sho108\\hydroml\\examples\\notebooks\\..\\sample_data\\model\\version_0\\params.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sho108\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hydroml-dFLAodHf-py3.11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:617: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>finetuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nse</th>\n",
       "      <td>0.662218</td>\n",
       "      <td>0.890566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kge</th>\n",
       "      <td>0.602030</td>\n",
       "      <td>0.829118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.541452</td>\n",
       "      <td>0.308190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias</th>\n",
       "      <td>1.320835</td>\n",
       "      <td>0.866343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relative_bias</th>\n",
       "      <td>0.320835</td>\n",
       "      <td>-0.133657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolute_bias</th>\n",
       "      <td>1.320835</td>\n",
       "      <td>0.866343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nse_sqrt</th>\n",
       "      <td>0.720300</td>\n",
       "      <td>0.785461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               original  finetuned\n",
       "nse            0.662218   0.890566\n",
       "kge            0.602030   0.829118\n",
       "rmse           0.541452   0.308190\n",
       "bias           1.320835   0.866343\n",
       "relative_bias  0.320835  -0.133657\n",
       "absolute_bias  1.320835   0.866343\n",
       "nse_sqrt       0.720300   0.785461"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to adjust batch size to be able to fit the model in the memory. device='cpu' if no gpu is available.\n",
    "# When no layer_to_finetune is provided, all paarameters in the model are tuned.\n",
    "p,v = run_finetune_from_timeseries(model_path, cal_dynamic_data, static_data, catchment_id, device='cpu', batch_size=128)\n",
    "finetuned_model_path = Path(p) / v\n",
    "simulation_using_finetuned_model = run_hydrological_simulation(finetuned_model_path, val_dynamic_data, static_data, catchment_id, **kwargs)\n",
    "\n",
    "\n",
    "metrics =pd.concat([get_metrics(simulation_using_original_model), get_metrics(simulation_using_finetuned_model)]).T\n",
    "metrics.columns = ['original', 'finetuned']\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params.yaml\n",
      "Transforming data: loading transform parameters from ..\\sample_data\\model\\version_0\\params.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sho108\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hydroml-dFLAodHf-py3.11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:617: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\sho108\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hydroml-dFLAodHf-py3.11\\Lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "c:\\Users\\sho108\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hydroml-dFLAodHf-py3.11\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory \\\\fs1-cbr.nexus.csiro.au\\{ev-ca-macq}\\work\\sho108\\hydroml\\examples\\sample_data\\model\\version_0\\finetune_head_dynamic_embedding\\401208\\241217112210_12b1 exists and is not empty.\n",
      "\n",
      "  | Name              | Type       | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | static_embedding  | Linear     | 15     | train\n",
      "1 | dynamic_embedding | Linear     | 6      | train\n",
      "2 | lstm              | LSTM       | 266 K  | train\n",
      "3 | dropout           | Identity   | 0      | train\n",
      "4 | head              | Sequential | 2.6 K  | train\n",
      "---------------------------------------------------------\n",
      "2.6 K     Trainable params\n",
      "266 K     Non-trainable params\n",
      "268 K     Total params\n",
      "1.075     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\sample_data\\model\\version_0\\finetune_head_dynamic_embedding\\401208\\241217112210_12b1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ab69587a2f43caad264ffd2fa532c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming data: loading transform parameters from p:\\work\\sho108\\hydroml\\examples\\notebooks\\..\\sample_data\\model\\version_0\\params.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sho108\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\hydroml-dFLAodHf-py3.11\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:617: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 10 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# we need to adjust batch size to be able to fit the model in the memory. device='cpu' if no gpu is available. \n",
    "# This tune only the parameters in the layers_to_finetune.\n",
    "p,v = run_finetune_from_timeseries(model_path, cal_dynamic_data, static_data, catchment_id, device='cpu', batch_size=128, max_epochs=20, layers_to_finetune=['head', 'dynamic_embedding'])\n",
    "partial_finetuned_model_path = Path(p) / v\n",
    "simulation_using_partial_finetuned_model = run_hydrological_simulation(partial_finetuned_model_path, val_dynamic_data, static_data, catchment_id, **kwargs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the metrics\n",
    "\n",
    "To compare the performance of the different models we calculate the metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>partial_finetuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nse</th>\n",
       "      <td>0.662218</td>\n",
       "      <td>0.890566</td>\n",
       "      <td>0.827960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kge</th>\n",
       "      <td>0.602030</td>\n",
       "      <td>0.829118</td>\n",
       "      <td>0.888905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.541452</td>\n",
       "      <td>0.308190</td>\n",
       "      <td>0.386417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias</th>\n",
       "      <td>1.320835</td>\n",
       "      <td>0.866343</td>\n",
       "      <td>1.054681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relative_bias</th>\n",
       "      <td>0.320835</td>\n",
       "      <td>-0.133657</td>\n",
       "      <td>0.054681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absolute_bias</th>\n",
       "      <td>1.320835</td>\n",
       "      <td>0.866343</td>\n",
       "      <td>1.054681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nse_sqrt</th>\n",
       "      <td>0.720300</td>\n",
       "      <td>0.785461</td>\n",
       "      <td>0.839433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               original  finetuned  partial_finetuned\n",
       "nse            0.662218   0.890566           0.827960\n",
       "kge            0.602030   0.829118           0.888905\n",
       "rmse           0.541452   0.308190           0.386417\n",
       "bias           1.320835   0.866343           1.054681\n",
       "relative_bias  0.320835  -0.133657           0.054681\n",
       "absolute_bias  1.320835   0.866343           1.054681\n",
       "nse_sqrt       0.720300   0.785461           0.839433"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics =pd.concat([get_metrics(simulation_using_original_model), get_metrics(simulation_using_finetuned_model), get_metrics(simulation_using_partial_finetuned_model)]).T\n",
    "metrics.columns = ['original', 'finetuned', 'partial_finetuned']\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydroml-dFLAodHf-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
